Mini GPT â€” Train a Transformer From Scratch (BPE Tokenizer + PyTorch)

This project implements a minimal GPT-style language model from scratch using:
	â€¢	Byte Pair Encoding (BPE) tokenizer
	â€¢	Decoder-only Transformer
	â€¢	Autoregressive next-token prediction
	â€¢	PyTorch

It is designed to be small enough to train on a MacBook CPU, while still exposing the core concepts used in modern LLMs like GPT-2/GPT-3.

â¸»

ðŸ”¥ Features

âœ” Train a custom BPE tokenizer

Using the tokenizers library, the model learns subword units that balance flexibility and efficiency.

âœ” Implement a full GPT-style architecture
	â€¢	Token embeddings
	â€¢	Positional embeddings
	â€¢	Multi-head self-attention
	â€¢	Feed-forward blocks
	â€¢	LayerNorm + residual connections
	â€¢	Causal masking

âœ” Observe training dynamics
	â€¢	Loss curve
	â€¢	Overfitting behavior
	â€¢	Generated text samples
	â€¢	Impact of context length and model capacity

âœ” Fully runnable on CPU

No GPU required.